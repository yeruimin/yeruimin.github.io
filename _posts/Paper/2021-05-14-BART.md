---
title: BART:基于自然语言生成、翻译、理解任务的去噪声Seq2Seq预训练
categories: [Paper]
comments: true
---

参考资料：
+ [BART原文地址](https://arxiv.org/pdf/1910.13461.pdf)
+ [ACL2020-BART：请叫我文本生成领域的老司机_模型](https://www.sohu.com/a/420794770_464065)
+ [BART原理简介与代码实战 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/121788986)
+ [Transformer与BERT浅说 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/49542105)
+ [【NLP】Facebook提出的预训练模型BART](https://cloud.tencent.com/developer/article/1543802)

## 基本信息

+ **期刊：ACL**（ACL2020年引用量最高文章）
+ **年份：2020年**
+ **作者**：Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer（**Facebook AI**）

## 研究问题

+ **BART：预训练sequence-to-sequence的去噪自编码器**：  
BART 使用**基于 Transformer 的标准神经机器翻译架构**，即BERT(双向编码器)、GPT(从左至右的解码器)以及其他近期出现的预训练模型的泛化形式

+ **BART训练步骤**：
    1. 用**任意噪声**破坏原文本
    2. **学习模型**来重构原文本

