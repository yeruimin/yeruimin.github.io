---
title: BTM-针对短文本的Biterm主题建模
categories: [paper]
comments: true
---

## 基本信息

+ **期刊：WWW**（Proceedings of the 22nd international conference on World Wide Web第22届国际万维网会议）
+ **发表年份：2013年**
+ **作者**：Xiaohui Yan, Jiafeng Guo, Yanyan Lan, Xueqi Cheng（中国科学院计算技术研究所）

## 研究问题

+ **研究适用于短文本的主题模型：**

**短文本主题建模**已经成为内容分析应用的一项重要任务。本文提出了一种新的短文本主题建模方法，即Biterm主题模型(BTM)。区别于LDA主题模型，在BTM中，通过直接建模**整个语料库中单词共现模式**(即biterm)的来进行主题建模。实际数据结果表明，BTM模型可以发现更突出和一致的主题，并在相应的评价指标上**明显优于baseline**。此外，BTM甚至可以在**普通文本**上优于LDA，显示了BTM的**潜在通用性**和更广泛的使用范围。

## 动机

+ **短文本主题建模的广泛需求**：

无论是传统的网站，如网页标题、文字广告和图片说明，还是新兴的社交媒体，如推特、状态信息和问答网站中的问题，**短文本在网络上非常流行**。揭示这样的短文本的主题对于广泛的内容分析任务是非常重要的，如内容特征分析，用户兴趣分析，新兴主题检测等等。

+ **短文本区别于普通文本的特点**：

传统的主题模型通过隐式捕获文档单词共现模式（**词袋模型**）来揭示主题，而在短文本中，存在严重的**数据稀疏性**问题，影响建模的效果。
1. 短文本中的单词**差异性**比在普通文本更小。在普通文本中，模型有足够的信息来知道单词是如何相互联系的;
2. 有限的语境使得主题模型在较短的文本中更难识别**歧义词**的含义；

+ **已有研究的缺陷**：

针对短文本的主题建模主要分为三类：

1. 将多个短文本**聚合**为一个普通长文本。例如：基于用户的文本聚合，基于相同关键词的文本聚合。缺陷：这种启发式数据聚合方法是**高度依赖数据**的。
2. 对数据的生成模式做出**更强的假设**。例如：假设一个短文本只涵盖一个主题。缺陷：失去了在一个文档中不同词语来自不同主题的**灵活性**。
3. 利用外部资源学习得到主题模型，用过**迁移学习**来学习短文本的主题。缺陷：**过度依赖外部资源**

+ **本文的主要贡献有**：

1. 提出一种基于**词共现模式建模的主题模型**，适用于**一般领域**的短文本主题建模。
2. 实验结果表明，在**所有评价指标**上，BTM模型都优于已有研究的结果。
3. 在普通文本上，BTM模型也有不差于LDA模型的表现，说明BTM模型的**潜在通用性**。


## 模型

+ **BTM的核心思想**：

基于整个语料库中的Biterm来学习短文本中的主题，以解决单个文档的稀疏性问题。语料库是一个主题的混合，每一个词来自一个特定的主题。如果一个biterm对中的两个词都来自同一个主题，那么，这个biterm对来自一个确定的主题的概率变大。

+ **Biterm对的解释**：

Biterm对表示一个短上下文中共现的词的词组。短上下文表示合适窗宽的词共现。

在短文本中，**每个文档作为一个上下文单元**。在普通文档中，选定一个合适的**窗宽**，窗口内的所有词作为一个上下文单元。

**【例】“清风明月，草长莺飞，杨柳依依“**

> ['清风明月','草长莺飞','杨柳依依']

**则这句话包含三个Biterm对**：

> ['清风明月','草长莺飞'] ['杨柳依依','草长莺飞']['清风明月','杨柳依依']

**LDA**：假设“清风明月”属于主题一，“杨柳依依”属于主题二，“草长莺飞”属于主题三

**BTM**：假设['清风明月','草长莺飞']属于主题一，['杨柳依依','草长莺飞']属于主题二，['清风明月','杨柳依依']属于主题三

+ **BTM生成过程**：

1. 对于每个主题z：
	生成一个主题词分布：$\phi_z\simDir(\beta)$
2. 对整个语料库生成主题分布：$\theta\sim Dir(\alpha)$
3. 对于Biterm集合B中的每个biterm对$b=(w_i,w_j)$：
	+ a)生成biterm对b的主题：$z\sim Multi(\theta)$
	+ b) 基于a）中生成的biterm对b的主题z，生成两个词：$w_i,w_j\sim Multi(\phi_z)$

+ **比较LDA、PLSA与BTM的区别**：

![(a)LDA模型；(b)mixture of unigrams模型；(c)BTM模型；](/asserts/img/BTM/pic1.png "LDA、PLSA与BTM的区别")

**LDA模型**：每个文档得到一个主题分布$\theta_d$，然后对文档中的每个单词$w$抽取一个主题$z$。

**Mixture of Unigrams模型**：从语料库的主题分布$\theta$中为每个文档分配一个主题$z$。

**BTM模型**：从语料库的主题分布$\theta$得到Biterm对$b=(w_i,w_j)$的主题$z$。
